\subsection{\acs*{cadx}: Feature selection and feature extraction} \label{subsec:chp3:img-clas:CADX-fea-ext}
As presented in the previous section, it is a common practise to extract a wide variety of features.
While dealing with \ac{mpmri}, the feature space created is a high-dimensional space which might mislead or corrupt the classifier during the training phase.
Therefore, it is of interest to reduce the number of dimensions before proceeding to the classification task.
The strategies used can be grouped as: (i) feature selection and (ii) feature extraction.
In this section only the methods used in \ac{cad} for \ac{cap} systems are presented.

\subsubsection{Feature selection}\label{subsubsec:chp3:img-clas:CADX:fea-ext:sel}
The feature selection strategy is based on selecting the most discriminative feature dimensions of the high-dimensional space.
Thus, the low-dimensional space is then composed of a subset of the original features detected.
In this section, methods employed in \ac{cad} for \ac{cap} detection are presented.
A more extensive review specific to feature selection is available in~\cite{Saeys2007}.

\citeauthor{Niaf2012} make use of the p-value by using the independent two-sample t-test with equal mean for each feature dimension~\cite{Niaf2011,Niaf2012}.
In this statistical test, there are 2 classes: \ac{cap} and healthy tissue.
Hence, for each particular feature, the distribution of each class is characterized by their means $\bar{X}_1$ and $\bar{X}_2$ and standard deviation $s_{X_1}$ and $s_{X_2}$.
Therefore, the null hypothesis test is based on the fact that these both distribution means are equal.
The t-statistic used to verify the null hypothesis is formalized such that:

\begin{eqnarray}
t & = & \frac{\bar {X}_1 - \bar{X}_2}{s_{X_1X_2} \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} \ , \label{eq:tstat} \\
s_{X_1X_2} & = & \sqrt{\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}} \ , \nonumber
\end{eqnarray}

\noindent where $n_1$ and $n_2$ are the number of samples in each class.
From \acs{eq}\,\eqref{eq:tstat}, more the means of the class distribution diverge, the larger the $t$-statistic $t$ will be, implying that this particular feature is more relevant and able to make the distinction between the two classes. 

The $p$-value statistic is deduced from the $t$-test and corresponds to the probability of obtaining such an extreme test assuming that the null hypothesis is true~\cite{Goodman1999}.
%Hence, smaller the $p$-value, the more likely we are to reject the null hypothesis and more relevant the feature is likely to be.
Hence, smaller the $p$-value, the more likely the null hypothesis to be rejected and more relevant the feature is likely to be.
Finally, the features are ranked and the most significant features are selected.
However, this technique suffers from a main drawback since it assumes that each feature is independent, which is unlikely to happen and introduces a high degree of redundancy in the features selected.

\citeauthor{Vos2012} in~\cite{Vos2012} employed a similar feature ranking approach but make use of the Fisher discriminant ratio to compute the relevance of each feature dimension.
Taking the aforementioned formulation, the Fisher discriminant ratio is formalized as the ratio of the interclass variance to the intraclass variance as:

\begin{equation}
F_r = \frac{(\bar{X}_1 - \bar{X}_2)^2}{s^{2}_{X_1}+s^{2}_{X_2}} \ .
\label{eq:fisherratio}
\end{equation}

Therefore, a relevant feature dimension is selected when the interclass variance is maximum and the intraclass variance in minimum.
Once the features are ordered, the authors select the feature dimensions with the largest Fisher discriminant ratio.

\Ac{mi} is a possible metric to use for selecting a subset of feature dimensions.
This method has previously been presented in \acs{sec}\,\ref{subsec:chp3:img-reg:reg} and expressed in \acs{eq}\,\eqref{eq:midef}.
%%%% As previously presented in Sect.~\ref{subsec:chp3:img-reg:reg} (see Eq.~\eqref{eq:midef}), the computation of the entropies involves the estimation of some \acp{pdf} and the data being usually continous variables, it is then necessary to estimate the \acp{pdf} using a method such as Parzen windows.
%%%% Definition of the \ac{mi} was presented in Sect.~\ref{subsec:chp3img-reg:reg} and formalized in Eq. \eqref{eq:midef} and as previously mentioned, the computation of the entropies involves the estimation of some \acp{pdf} and the data being usually continuous variables, it is then necessary to estimate the \acp{pdf} using a method such as Parzen windows.
\citeauthor{Peng2005}~\cite{Peng2005} introduced two main criteria to select the feature dimensions based on \ac{mi}: (i) maximal relevance and (ii) minimum redundancy.
Maximal relevance criterion is based on the paradigm that the classes and the feature dimension which has to be selected have to share a maximal \ac{mi} and is formalized as:
\begin{equation}
  \argmax Rel(\mathbf{x},c) = \frac{1}{|\mathbf{x}|} \sum_{x_i \in \mathbf{x}} MI(x_i,c)  \ , 
  \label{eq:mRel}
\end{equation}
\noindent where $\mathbf{x} = \{x_i; i=1,\cdots,d\}$ is a feature vector of $d$ dimensions and $c$ is the class considered.
As in the previous method, using maximal relevance criterion alone imply an independence between each feature dimension.
The minimal redundancy criterion enforce the selection of a new feature dimension which shares as little as possible \ac{mi} with the previously selected feature dimensions such that:
\begin{equation}
  \argmin Red(\mathbf{x}) = \frac{1}{|\mathbf{x}|^2} \sum_{x_i,x_j \in \mathbf{x}} MI(x_i,x_j)  \ . 
  \label{eq:mRed}
\end{equation}
Combination of these two criteria is known as the \ac{mrmr} algorithm~\cite{Peng2005}.
Two combinations are usually used: (i) the difference or (ii) the quotient.
This method has been used at several occasions for the selecting a subset of features prior to classification~\cite{Niaf2011,Niaf2012,lehaire2014computer,Viswanath2012,khalvati2015automated,chung2015prostate}.

\subsubsection{Feature extraction}\label{subsubsec:chp3:img-clas:CADX:fea-ext:ext}
The feature extraction strategy is related to dimension reduction methods but not selecting discriminative features.
Instead, these methods aim at mapping the data from the high-dimensional space into a low-dimensional space to maximize the separability between the classes.
As in the previous sections, only methods employed in \ac{cad} system are reviewed in this section.
We refer the reader to~\cite{Fodor2002} for a full review of feature extraction techniques.

\ac{pca} is the most commonly used linear mapping method in \ac{cad} systems.
\ac{pca} is based on finding the orthogonal linear transform mapping the original data into a low-dimensional space.
The space is defined such that the linear combinations of the original data with the $k^{th}$ greatest variances lie on the $k^{th}$ principal components~\cite{Jolliffe2002}.
The principal components are computed by using the eigenvectors-eigenvalues decomposition of the covariance matrix.
Let $\mathbf{x}$ denote the data matrix.
Then, the covariance matrix and eigenvectors-eigenvalues decomposition are defined as in \acs{eq}\,\eqref{eq:covmat}, and \acs{eq}\,\eqref{eq:eigpca}, respectively. 
The eigenvectors-eigenvalues decomposition can be formalized as:
\begin{equation}
  \Sigma = \mathbf{x}^{\text{T}} \mathbf{x} \ .
  \label{eq:covmat}
\end{equation}

\begin{equation}
  \mathbf{v}^{-1} \Sigma \mathbf{v} = \Lambda \ ,
  \label{eq:eigpca}
\end{equation}
\noindent where $\mathbf{v}$ are the eigenvectors matrix and $\Lambda$ is a diagonal matrix containing the eigenvalues. 

It is then possible to find the new low-dimensional space by sorting the eigenvectors using the eigenvalues and finally select the eigenvectors corresponding to the largest eigenvalues.
The total variation that is the sum of the principal eigenvalues of the covariance matrix~\cite{Fodor2002}, usually corresponds to the \SIrange{95}{98}{\percent} of the cumulative sum of the eigenvalues.
\citeauthor{Tiwari2012} used \ac{pca} in order to reduce the complexity of feature space~\cite{Tiwari2008,Tiwari2009,Tiwari2012}.

Non-linear mapping has been also used for dimension reduction and is mainly based on Laplacian eigenmaps and \acf{lle} methods.
Laplacian eigenmaps also referred as spectral clustering in computer vision, aim to find a low-dimensional space in which the proximity of the data should be preserved from the high-dimensional space~\cite{Shi2000,Belkin2001}.
Therefore, two adjacent data points in the high-dimensional space should also be close in the low-dimensional space.
Similarly, two distant data points in the high-dimensional space should also be distant in the low-dimensional space.
To compute this projection, an adjacency matrix is defined as:
\begin{equation}
	W(i,j) = \exp \| \mathbf{x}_i - \mathbf{x}_j \|_2 \ ,
	\label{eq:gew}
\end{equation}

\noindent where $\mathbf{x}_i$ and $\mathbf{x}_j$ are the two samples considered.
Then, the low-dimensional space is found by solving the generalized eigenvectors-eigenvalues problem:

\begin{equation}
	(D-W)\mathbf{y} = \lambda D \mathbf{y} \ ,
	\label{eq:geeig}
\end{equation}

\noindent where $D$ is a diagonal matrix such that $D(i,i) = \sum_j W(j,i)$.
Finally the low-dimensional space is defined by the $k$ eigenvectors of the $k$ smallest eigenvalues~\cite{Belkin2001}.
\citeauthor{Tiwari2009a}~\cite{Tiwari2007,Tiwari2009,Tiwari2009a} and \citeauthor{Viswanath2008}~\cite{Viswanath2008} used this spectral clustering to project their feature vector into a low-dimensional space.
The feature space in these studies is usually composed of features extracted from a single or multiple modalities and then concatenated before applying the Laplacian eigenmaps dimension reduction technique.

\citeauthor{Tiwari2013} used a slightly different approach by combining the Laplacian eigenmaps techniques with a prior multi-kernel learning strategy~\cite{Tiwari2009,Tiwari2013}.
First, multiple features are extracted from multiple modalities.
The features of a single modality are then mapped to a higher-dimensional space via the Kernel trick~\cite{Aizerman1964}, namely a Gaussian kernel.
Then, each kernel is linearly combined to obtain a combined kernel $K$ and the adjacency matrix $W$ is computed.
Finally, the same scheme as in the Laplacian eigenmaps is applied.
However, in order to use the combined kernel, \acs{eq}\,\eqref{eq:geeig} is rewritten as:

\begin{equation}
  K (D-W) K^{\text{T}} \mathbf{y} = \lambda K D K^{\text{T}} \mathbf{y} \ ,
  \label{eq:sesmik}
\end{equation}
\noindent which is solved as a generalized eigenvectors-eigenvalues problem as previously.
\citeauthor{Viswanath2011} used Laplacian eigenmaps inside a bagging framework in which multiple embeddings are generated by successively selecting feature dimensions~\cite{Viswanath2011}.

\Ac{lle} is another common non-linear dimension reduction technique widely used, first proposed in~\cite{Roweis2000}.
\ac{lle} is based on the fact that a data point in the feature space is characterized by its neighbourhood.
Thus, each data point in the high-dimensional space is transformed to represent a linear combination of its $k$-nearest neighbours.
This can be expressed as:
\begin{equation}
	\hat{\mathbf{x}}_i = \sum_j W(i,j) \mathbf{x}_j \ ,
	\label{eq:lincomlle}
\end{equation}

\noindent where $\hat{\mathbf{x}}_i$ are the data points estimated using its neighbouring data points $\mathbf{x}_j$, and $W$ is the weight matrix.
The weight matrix $W$ is estimated using a least square optimization as in \acs{eq}\,\eqref{eq:lslle}.
%Hence, this problem which has to be solved at this stage is to estimate the weight matrix $W$. This problem can be tackled using a least square optimization scheme by optimizing the following objective function:
\begin{eqnarray}
	\hat{W} & = & \argmin_{W} \sum_i | \mathbf{x}_i - \sum_j W(i,j)\mathbf{x}_j |^{2} \ , \label{eq:lslle} \\
	&& \text{subject to } \sum_j W(i,j) = 1 \ , \nonumber
\end{eqnarray}

Then, the essence of \ac{lle} is to project the data into a low-dimensional space, while retaining the data spatial organization.
Therefore, the projection into the low-dimensional space is tackled as an optimization problem as:

\begin{equation}
	\hat{\mathbf{y}} = \argmin_{\mathbf{y}} \sum_i | \mathbf{y}_i - \sum_j W(i,j)\mathbf{y}_j |^{2} \ .
	\label{eq:lowprojlle}
\end{equation}

This optimization is solved as an eigenvectors-eigenvalues problem by finding the $k^{\text{th}}$ eigenvectors corresponding to the $k^{\text{th}}$ smallest eigenvalues of the sparse matrix $(I-W)^{\text{T}}(I-W)$.

\citeauthor{Tiwari2008} used a modified version of the \ac{lle} algorithm in which they applied \ac{lle} in a bagging approach with multiple neighbourhood sizes~\cite{Tiwari2008}.
The different embeddings obtained are then fused using the \ac{ml} estimation.

Another way of reducing the complexity of high-dimensional feature space is to use the family of so-called dictionary-based methods.
\Ac{scf} representation has become very popular in other computer vision application and has been used by \citeauthor{lehaire2014computer} in~\cite{lehaire2014computer}.
The main goal of sparse modeling is to efficiently represent the images as a linear combination of a few typical patterns, called atoms, selected from a dictionary.
Sparse coding consists of three main steps: sparse approximation, dictionary learning, and low-level features projection~\cite{rubinstein2008efficient}.

\emph{Sparse approximation -} Given a dictionary $\mathbf{D} \in \mathbb{R}^{n \times K}$ composed of $K$ atoms and an original signal $\mathbf{y} \in \mathbb{R}^{n}$ --- i.e., one feature vector ---, the sparse approximation corresponds to find the sparest vector $\mathbf{x} \in \mathbb{R}^{K}$ such that:

\begin{equation}
  \argmin_{\mathbf{x}}\|\mathbf{y - Dx} \|_{2} \qquad  \text{s.t.} \  \|\mathbf{x}\|_{0} \leq \lambda \, \label{eq:sprapp} \ ,
\end{equation}
\noindent where $\lambda$ is a specified sparsity level.

Solving the above optimization problem is an NP-hard problem~\cite{elad2010sparse}.
However, approximate solutions are obtained using greedy algorithms such as \ac{mp}~\cite{mallat1993matching} or \ac{omp}~\cite{pati1993orthogonal,davis1997adaptive}.

\emph{Dictionary learning -} As stated previously, the sparse approximation is computed given a specific dictionary $\mathbf{D}$, which involves a learning stage from a set of training data.
This dictionary is learned using $K$-\acs*{svd} which is a generalized version of $K$-means clustering and uses \ac{svd}. 
The dictionary is built, in an iterative manner by solving the optimization problem of \acs{eq}\,\eqref{eq:dct}, by alternatively computing the sparse approximation of $\mathbf{X}$ and the dictionary $\mathbf{D}$.
\begin{equation}
  \argmin_{\mathbf{D,X}} \|\mathbf{Y} - \mathbf{D}\mathbf{X}\|_{2} \qquad  \text{s.t.} \  \|\mathbf{x}_{i}\|_{1} \leq \lambda \,\label{eq:dct} \ ,
\end{equation}
\noindent where $\mathbf{Y}$ is a training set of low-level descriptors, $\mathbf{X}$ is the associated sparse coded matrix --- i.e., set of high-level descriptors --- with a sparsity level $\lambda$, and $\mathbf{D}$ is the dictionary with $K$ atoms.
Given $\mathbf{D}$, $\mathbf{X}$ is computed using the batch-\ac{omp} algorithm, while given $\mathbf{X}$, $\mathbf{D}$ is sequentially updated, one atom at a time using \ac{svd}. 

\emph{Low-level features projection -} Once the dictionary is learned, each set of low-level features $\mathbf{F}_{I}$ previously extracted is encoded using the dictionary $\mathbf{D}$, solving the optimization problem presented in \acs{eq}\,\eqref{eq:sprapp} such that $\mathbf{F}_{I} \simeq \mathbf{DX}_{I}$.

The \ac{bow} approach offers an alternative method~\cite{Sivic2003} for feature extraction.
\Ac{bow} was used by \citeauthor{rampun2016computerb} in~\cite{rampun2015classifying,rampun2016computerb}.
This model represents the features by creating a codebook or visual dictionary, from the set of low-level features.
The set of low-level features are clustered using \textit{k}-means to create the dictionary with \textit{k} clusters known as visual words.
Once the codebook is created from the training set, the low-level descriptors are replaced by their closest word within the codebook.
The final descriptor is a histogram of size \textit{k} which represents the codebook occurrences for a given mapping.

% THIS IS FROM MY THESIS, YOU CAN USE THIS, If you like

%% \acf{scf}, or sparse signal representation, has become very popular over the past few decades and has led to state-of-the-art results in various applications such as face recognition~\cite{wright2009robust}, image denoising, image inpainting~\cite{elad2006image}, and image classification~\cite{sidibe2015discrimination}. 
%% The main goal of sparse modeling is to efficiently represent the samples/images as a linear combination of a few typical patterns, called atoms, selected from the dictionary.
%% Sparse coding consists of three main steps: (i) dictionary learning, (ii) low-level feature projection, and (iii) feature pooling~\cite{rubinstein2008efficient}. 
%% Considering our dictionary $D \in R^{n \times K}$ with $K$ atoms, where each column of $D$ represents one atom, the sparse coding problem of a signal $y \in R^{n}$ is defined as finding the sparsest vector $x$ so that $y \approx Dx$. 
%% This is an optimization problem that can be formulated as:
%% \begin{equation}
%% \min_{x} \|y-Dx\|_{2} \qquad \text{s.t.}\ \|x\|_{0} \leq \lambda \,,
%% \end{equation}  
%% \noindent where $\lambda$ is the sparsity level and $l^{0}$-norm accounts for the minimum number of non-zero elements in the sparse vector $x$. 
%% This optimization problem is NP hard~\cite{Elad2010}, subsequently approximation solutions are proposed either by using greedy algorithms such as \ac{mp}~\cite{mallat1993matching} and \ac{omp}~\cite{davis1994adaptive}, or by replacing the $l^{0}$-norm with $l^{1}$-norm such as in the \ac{bp} algorithm~\cite{chen1998atomic}.
%% %\noindent where $\lambda$ is the sparsity level and $l^{0}$-norm accounts for the minimum number of non-zero elements in sparse vector $x$. 
%% %This optimization problem is NP hard~\cite{Elad2010}.
%% %Subsequently approximation solutions are proposed either by using greedy algorithms such as \ac{mp}~\cite{mallat1993matching} and \ac{omp}~\cite{davis1994adaptive} or by replacing the $l^{0}$-norm with $l^{1}$-norm such as \ac{bp}~\cite{chen1998atomic}.
%% The dictionary is learned using K-SVD, a generalized version of \textit{k}-means clustering that uses \ac{svd}~\cite{aharon2006img}. 
%% The dictionary is built such that:
%% \begin{equation}
%%   \min_{Dx} \|y - Dx\|_{2} \qquad  \text{s.t.} \ \forall i \ \|x_{i}\|_{1} \leq \lambda \,,
%% \end{equation}

%% \noindent where $y$ is a low-level descriptor, $x$ is the sparse coded descriptor (i.e., high-level descriptor) with a sparsity level $\lambda$, and $D$ is the dictionary with $K$ atoms.
%% The K-SVD algorithm solves the optimization problem iteratively by alternating between $x$ and $D$. 
%% With $D$, the sparse code matrix $x$ is computed by any of the pursuit algorithms, and with $x$, $D$ is updated one atom at a time using \ac{svd}. 

%% Once the dictionary is learned, each $y_{i} \in R^{n}$ signal can be projected using $D$ to form a set of sparse codes $x_{i} \in R^{K}$. 
%% In the case of image samples, the sparse representation can be generated for patches in the image.
%% In this case, the final mapping is based on a combination of sparse codes, for instance by taking the maximum code from all the patches: 
%% \begin{equation}
%% f_{i} = \max_{j}(\vert X_{l}(i,j)\vert) \qquad  \forall  i = 1, 2, .., K \,,
%% \end{equation} 
%% \noindent where $X_{l} \in R^{K \times P}$ is the sparse code matrix~\cite{sidibe2015discrimination}. 
%% \end{description}

\subsubsection{Summary}

The feature selection and extraction used in \ac{cad} systems are summarized in \acs{tab}~\ref{tab:featext}.

\begin{table}
  \caption{Overview of the feature selection and extraction methods used in \acs*{cad} systems.}
  \scriptsize
  \centering
  \begin{tabular}{l r}
    \toprule
    \textbf{Dimension reduction methods} & \textbf{References} \\
    \midrule
    \textbf{Feature selection:} & \\ \\ [-1.5ex]
    \quad Statistical test & \cite{Niaf2011,Niaf2012,Vos2012} \\
    \quad \ac{mi}-based methods & \cite{Niaf2011,Niaf2012,Vos2008,lehaire2014computer,khalvati2015automated,chung2015prostate} \\
    \quad Correlation-based methods & \cite{rampun2016computer,rampun2015computer} \\ \\ [-1.5ex]
    \textbf{Feature extraction:} & \\ \\ [-1.5ex]
    \quad Linear mapping & \\
    \quad \quad \acs*{pca} & \cite{Tiwari2008,Tiwari2009} \\
    \quad Non-linear mapping & \\
    \quad \quad Laplacian eigenmaps & \cite{Tiwari2007,Tiwari2009a,Tiwari2009,Tiwari2010,Viswanath2008,Viswanath2011} \\
    \quad \quad \acs*{lle} and \acs*{lle}-based & \cite{Tiwari2008,Tiwari2009,Viswanath2008a,Viswanath2008} \\
    \quad Dictionary-based learning & \\
    \quad \quad Sparse coding & \cite{lehaire2014computer} \\
    \quad \quad \acs*{bow} & \cite{rampun2016computerb,rampun2015classifying} \\
    \bottomrule
  \end{tabular}
\label{tab:featext}
\end{table} 
